# BERT-Like-Large-language-models-in-Patient-Safety-and-Pharmacovigilance-PSPV

# Introduction

Objective: Enhance safety signal detection, adverse reaction management, and regulatory submissions in patient safety and pharmacovigilance (PSPV) using BERT-like Large Language Models (LLMs).
Highlight:
Dive deep into the application of generic pretrained, domain-specific pretrained, and safety knowledge-fine-tuned BERT-like LLMs for PSPV.
Investigate factors including data complexity, BERT model size, and domain-specific training impact.
Key Insights:
BERT-like LLMs consistently predict across different data complexities.
Bigger BERT model doesn't always mean better causal inference.
Domain-tailored models, whether fine-tuned or not, outperform generic pretrained BERT for causal inference in PSPV.
This research aims to offer a foundation for the future integration of LLMs in diverse PSPV applications.
